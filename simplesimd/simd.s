// Code generated by command: go run asm.go -out simd.s -stubs stub.go. DO NOT EDIT.

#include "textflag.h"

// func andSIMD(a []byte, b []byte, res []byte)
TEXT ·andSIMD(SB), NOSPLIT, $0-72
	MOVQ   a_base+0(FP), AX
	MOVQ   b_base+24(FP), CX
	MOVQ   res_base+48(FP), DX
	MOVQ   a_len+8(FP), BX
	VXORPS Y0, Y0, Y0
	VXORPS Y1, Y1, Y1
	VXORPS Y2, Y2, Y2
	VXORPS Y3, Y3, Y3
	VXORPS Y4, Y4, Y4
	VXORPS Y5, Y5, Y5
	VXORPS Y6, Y6, Y6
	VXORPS Y7, Y7, Y7

blockloop:
	CMPQ    BX, $0x00000100
	JL      tail
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VMOVUPS 192(AX), Y6
	VMOVUPS 224(AX), Y7
	VPAND   (CX), Y0, Y0
	VPAND   32(CX), Y1, Y1
	VPAND   64(CX), Y2, Y2
	VPAND   96(CX), Y3, Y3
	VPAND   128(CX), Y4, Y4
	VPAND   160(CX), Y5, Y5
	VPAND   192(CX), Y6, Y6
	VPAND   224(CX), Y7, Y7
	VMOVUPS Y0, (DX)
	VMOVUPS Y1, 32(DX)
	VMOVUPS Y2, 64(DX)
	VMOVUPS Y3, 96(DX)
	VMOVUPS Y4, 128(DX)
	VMOVUPS Y5, 160(DX)
	VMOVUPS Y6, 192(DX)
	VMOVUPS Y7, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000100, BX
	JMP     blockloop

tail:
	RET

// func orSIMD(a []byte, b []byte, res []byte)
TEXT ·orSIMD(SB), NOSPLIT, $0-72
	MOVQ   a_base+0(FP), AX
	MOVQ   b_base+24(FP), CX
	MOVQ   res_base+48(FP), DX
	MOVQ   a_len+8(FP), BX
	VXORPS Y0, Y0, Y0
	VXORPS Y1, Y1, Y1
	VXORPS Y2, Y2, Y2
	VXORPS Y3, Y3, Y3
	VXORPS Y4, Y4, Y4
	VXORPS Y5, Y5, Y5
	VXORPS Y6, Y6, Y6
	VXORPS Y7, Y7, Y7

blockloop:
	CMPQ    BX, $0x00000100
	JL      tail
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VMOVUPS 192(AX), Y6
	VMOVUPS 224(AX), Y7
	VPOR    (CX), Y0, Y0
	VPOR    32(CX), Y1, Y1
	VPOR    64(CX), Y2, Y2
	VPOR    96(CX), Y3, Y3
	VPOR    128(CX), Y4, Y4
	VPOR    160(CX), Y5, Y5
	VPOR    192(CX), Y6, Y6
	VPOR    224(CX), Y7, Y7
	VMOVUPS Y0, (DX)
	VMOVUPS Y1, 32(DX)
	VMOVUPS Y2, 64(DX)
	VMOVUPS Y3, 96(DX)
	VMOVUPS Y4, 128(DX)
	VMOVUPS Y5, 160(DX)
	VMOVUPS Y6, 192(DX)
	VMOVUPS Y7, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000100, BX
	JMP     blockloop

tail:
	RET

// func andnotSIMD(a []byte, b []byte, res []byte)
TEXT ·andnotSIMD(SB), NOSPLIT, $0-72
	MOVQ   a_base+0(FP), AX
	MOVQ   b_base+24(FP), CX
	MOVQ   res_base+48(FP), DX
	MOVQ   a_len+8(FP), BX
	VXORPS Y0, Y0, Y0
	VXORPS Y1, Y1, Y1
	VXORPS Y2, Y2, Y2
	VXORPS Y3, Y3, Y3
	VXORPS Y4, Y4, Y4
	VXORPS Y5, Y5, Y5
	VXORPS Y6, Y6, Y6
	VXORPS Y7, Y7, Y7

blockloop:
	CMPQ    BX, $0x00000100
	JL      tail
	VMOVUPS (CX), Y0
	VMOVUPS 32(CX), Y1
	VMOVUPS 64(CX), Y2
	VMOVUPS 96(CX), Y3
	VMOVUPS 128(CX), Y4
	VMOVUPS 160(CX), Y5
	VMOVUPS 192(CX), Y6
	VMOVUPS 224(CX), Y7
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VPANDN  64(AX), Y2, Y2
	VPANDN  96(AX), Y3, Y3
	VPANDN  128(AX), Y4, Y4
	VPANDN  160(AX), Y5, Y5
	VPANDN  192(AX), Y6, Y6
	VPANDN  224(AX), Y7, Y7
	VMOVUPS Y0, (DX)
	VMOVUPS Y1, 32(DX)
	VMOVUPS Y2, 64(DX)
	VMOVUPS Y3, 96(DX)
	VMOVUPS Y4, 128(DX)
	VMOVUPS Y5, 160(DX)
	VMOVUPS Y6, 192(DX)
	VMOVUPS Y7, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000100, BX
	JMP     blockloop

tail:
	RET
